{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ffcb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model generated.\n",
      "True\n",
      "Epoch 1/3, Batch Loss: 0.6945424675941467\n",
      "Epoch 1/3, Batch Loss: 0.6724446415901184\n",
      "Epoch 1/3, Batch Loss: 0.6597050428390503\n",
      "Epoch 1/3, Batch Loss: 0.6217326521873474\n",
      "Epoch 1/3, Batch Loss: 0.6547929644584656\n",
      "Epoch 1/3, Batch Loss: 0.6267513632774353\n",
      "Epoch 1/3, Batch Loss: 0.6148400902748108\n",
      "Epoch 1/3, Batch Loss: 0.7364100813865662\n",
      "Epoch 1/3, Batch Loss: 0.6414968967437744\n",
      "Epoch 1/3, Batch Loss: 0.5776723027229309\n",
      "Epoch 1/3, Batch Loss: 0.6653754711151123\n",
      "Epoch 1/3, Batch Loss: 0.5628467798233032\n",
      "Epoch 1/3, Batch Loss: 0.5154410004615784\n",
      "Epoch 1/3, Batch Loss: 0.6589382886886597\n",
      "Epoch 1/3, Batch Loss: 0.7185307145118713\n",
      "Epoch 1/3, Batch Loss: 0.5818533897399902\n",
      "Epoch 1/3, Batch Loss: 0.6585012674331665\n",
      "Epoch 1/3, Batch Loss: 0.569675624370575\n",
      "Epoch 1/3, Batch Loss: 0.5187422633171082\n",
      "Epoch 1/3, Batch Loss: 0.6348533034324646\n",
      "Epoch 1/3, Batch Loss: 0.5761305689811707\n",
      "Epoch 1/3, Batch Loss: 0.5870919227600098\n",
      "Epoch 1/3, Batch Loss: 0.6113176345825195\n",
      "Epoch 1/3, Batch Loss: 0.558736264705658\n",
      "Epoch 1/3, Batch Loss: 0.4884689748287201\n",
      "Epoch 1/3, Batch Loss: 0.5871737599372864\n",
      "Epoch 1/3, Batch Loss: 0.8140737414360046\n",
      "Epoch 1/3, Batch Loss: 0.6045228242874146\n",
      "Epoch 1/3, Batch Loss: 0.5624098181724548\n",
      "Epoch 1/3, Batch Loss: 0.6012383103370667\n",
      "Epoch 1/3, Batch Loss: 0.5474061369895935\n",
      "Epoch 1/3, Batch Loss: 0.7525322437286377\n",
      "Epoch 1/3, Batch Loss: 0.5175954103469849\n",
      "Epoch 1/3, Batch Loss: 0.501681923866272\n",
      "Epoch 1/3, Batch Loss: 0.5292935371398926\n",
      "Epoch 1/3, Batch Loss: 0.6976707577705383\n",
      "Epoch 1/3, Batch Loss: 0.7190853953361511\n",
      "Epoch 1/3, Batch Loss: 0.4122632145881653\n",
      "Epoch 1/3, Batch Loss: 0.44881075620651245\n",
      "Epoch 1/3, Batch Loss: 0.6050453782081604\n",
      "Epoch 1/3, Batch Loss: 0.447398841381073\n",
      "Epoch 1/3, Batch Loss: 0.6066264510154724\n",
      "Epoch 1/3, Batch Loss: 0.4211738407611847\n",
      "Epoch 1/3, Batch Loss: 0.4476950168609619\n",
      "Epoch 1/3, Batch Loss: 0.569235622882843\n",
      "Epoch 1/3, Batch Loss: 0.8163866400718689\n",
      "Epoch 1/3, Batch Loss: 0.529228925704956\n",
      "Epoch 1/3, Batch Loss: 0.5783017873764038\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-71c3508cfe15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch + 1}/{epochs}, Batch Loss: {loss.item()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;31m# hidden state를 저장합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-5]  # n번째 레이어의 hidden states를 반환합니다.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data_A = pd.read_csv(\"output1.csv\")  # data set A 파일명에 맞게 수정\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B 파일명에 맞게 수정\n",
    "# 모델 저장 경로\n",
    "model_path = \"Pre-trained.pt\"\n",
    "\n",
    "# 중복된 환자 정보 제거\n",
    "data_A_unique = data_A.drop_duplicates(subset=\"ID\")\n",
    "\n",
    "# X_train, Y_train 생성\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A_unique.iterrows():\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "# BERT 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 모델이 이미 저장되어 있는지 확인하고, 저장된 모델이 있으면 불러오고 없으면 새로운 모델 생성\n",
    "if os.path.exists(model_path):\n",
    "    # 저장된 모델이 있을 경우 불러오기\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # 저장된 모델이 없을 경우 새로운 모델 생성\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# 입력 데이터를 BERT의 입력 형식으로 변환\n",
    "max_len = 128  # 입력 시퀀스의 최대 길이\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # 환자 정보 및 증상\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] 토큰 추가\n",
    "                        max_length = max_len,         # 최대 길이 지정\n",
    "                        pad_to_max_length = True,     # 패딩을 추가하여 최대 길이로 맞춤\n",
    "                        return_attention_mask = True, # 어텐션 마스크 생성\n",
    "                        return_tensors = 'pt',        # PyTorch 텐서로 반환\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 전체 데이터셋을 훈련 데이터셋으로 사용\n",
    "train_dataset = dataset\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 및 학습률 설정\n",
    "# 기본 학습률 : 2e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)\n",
    "\n",
    "# 에폭 설정\n",
    "epochs = 3\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden_states_list = []  # 각 배치의 hidden state를 저장할 리스트\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]  # loss가 outputs의 두 번째 값입니다.\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Batch Loss: {loss.item()}')\n",
    "        # hidden state를 저장합니다.\n",
    "        hidden_states = outputs[2]\n",
    "        hidden_states_list.append(hidden_states)\n",
    "\n",
    "    # 각 배치의 hidden state를 합쳐서 CSV 파일로 저장합니다.\n",
    "    hidden_states = torch.cat(hidden_states_list, dim=0)\n",
    "    hidden_states = hidden_states[:, 0, :].cpu().detach().numpy()\n",
    "    hidden_states_df = pd.DataFrame(hidden_states)\n",
    "    hidden_states_df.to_csv(f\"hidden_states_epoch{epoch + 1}.csv\", index=False)  # 각 epoch마다 파일 이름을 다르게 지정합니다.\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3a985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
